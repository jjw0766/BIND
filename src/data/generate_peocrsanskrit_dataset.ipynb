{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4840eb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e80a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "import json\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from grapheme import graphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae622b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_sanskrit(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     입력 문자열에서 데바나가리 문자(산스크리트)를 추출하고\n",
    "#     나머지 기호/영문/숫자 등은 제거한 뒤 반환합니다.\n",
    "#     \"\"\"\n",
    "#     # \\p{Devanagari}는 유니코드 스크립트 기반 정규식 (데바나가리 블록 전체)\n",
    "#     tokens = regex.findall(r'\\p{Devanagari}+', text)\n",
    "#     return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex\n",
    "\n",
    "# # 기본 정의\n",
    "# CONSONANT        = r'[\\u0915-\\u0939]'                    # क-ह\n",
    "# VIRAMA           = r'\\u094D'                             # ् (halant)\n",
    "# VOWEL_SIGN       = r'[\\u093E-\\u094C\\u0962\\u0963]'        # ा-ौ, ॢ ॣ\n",
    "# INDEPENDENT_VOWEL= r'[\\u0904-\\u0914]'                    # ऄ-औ (독립 모음)\n",
    "# DIACRITICS       = r'[\\u0900-\\u0903]'                    # ऀ, ँ, ं, ः\n",
    "# ZWJ_ZWNJ         = r'[\\u200C\\u200D]'                     # 합자 조절 ZWJ/ZWNJ\n",
    "\n",
    "# # 자음군 (conjunct consonants)\n",
    "# CONJUNCT = fr'{CONSONANT}(?:{VIRAMA}{ZWJ_ZWNJ}?{CONSONANT})*'\n",
    "\n",
    "# # 개선된 'अक्षर' 단위 정규식\n",
    "# AKSHARA_PATTERN = regex.compile(\n",
    "#     fr'(?:{INDEPENDENT_VOWEL}|{CONJUNCT}(?:{VOWEL_SIGN})?)(?:{DIACRITICS})?'\n",
    "# )\n",
    "\n",
    "# def is_valid_sanskrit_word(word: str) -> bool:\n",
    "#     \"\"\"\n",
    "#     산스크리트 문자열이 정상적인 '글자 단위(अक्षर, akṣara)'로만 구성되었는지 판별.\n",
    "#     - 독립 모음 (अ, आ, इ, ...)\n",
    "#     - 자음 단독, 자음 + 모음기호\n",
    "#     - 자음 결합 (halant/ZWJ/ZWNJ로 연결)\n",
    "#     - 발음 보조 기호 (अनुस्वार, विसर्ग, चन्द्रबिन्दु 등)\n",
    "#     - halant 종결도 허용 (क् 등)\n",
    "#     \"\"\"\n",
    "#     tokens = regex.findall(r'\\X', word)  # grapheme cluster 단위 분리\n",
    "    \n",
    "#     for t in tokens:\n",
    "#         if not AKSHARA_PATTERN.fullmatch(t):\n",
    "#             return False\n",
    "#     return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008242e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preproc_df(df):\n",
    "#     sentences = []\n",
    "#     sentences_noisy = []\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#         sentence_noisy = row['input_text'].strip()\n",
    "#         sentence = row['target_text'].strip()\n",
    "#         sentence_noisy_chunks = sentence_noisy.split('.')\n",
    "#         sentence_chunks = sentence.split('.')\n",
    "#         sentence_noisy_chunks = [sentence_noisy_chunk.strip() for sentence_noisy_chunk in sentence_noisy_chunks]\n",
    "#         sentence_chunks = [sentence_chunk.strip() for sentence_chunk in sentence_chunks]\n",
    "#         if len(sentence_noisy_chunks) != len(sentence_chunks):\n",
    "#             continue\n",
    "#         sentence_chunks_result = []\n",
    "#         sentence_noisy_chunks_result = []\n",
    "#         for sentence_noisy_chunk, sentence_chunk in zip(sentence_noisy_chunks, sentence_chunks):\n",
    "#             # sentence_noisy_chunk = extract_sanskrit(sentence_noisy_chunk.strip())\n",
    "#             sentence_chunk = extract_sanskrit(sentence_chunk.strip())\n",
    "#             sentence_noisy_chunks_result.append(sentence_noisy_chunk)\n",
    "#             sentence_chunks_result.append(sentence_chunk)\n",
    "#         sentence_noisy = '. '.join(sentence_noisy_chunks_result).strip()\n",
    "#         sentence = '. '.join(sentence_chunks_result).strip()\n",
    "        \n",
    "#         sentence_noisy_len = len(list(graphemes(sentence_noisy)))\n",
    "#         sentence_len = len(list(graphemes(sentence)))\n",
    "\n",
    "#         if sentence_len != sentence_noisy_len:\n",
    "#             continue\n",
    "\n",
    "#         break_flag = False\n",
    "#         for word in sentence.split(' '):\n",
    "#             if not is_valid_sanskrit_word(word):\n",
    "#                 break_flag = True\n",
    "#         if break_flag:\n",
    "#             continue\n",
    "\n",
    "#         sentences.append(sentence)\n",
    "#         sentences_noisy.append(sentence_noisy)\n",
    "#     return sentences, sentences_noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95223a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_max_check(sentence, n=3):\n",
    "    for char in graphemes(sentence):\n",
    "        if len(char)>n:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60bfd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_df(df):\n",
    "    sentences = []\n",
    "    sentences_noisy = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        sentence_noisy = row['input_text'].strip()\n",
    "        sentence = row['target_text'].strip()\n",
    "        sentence_noisy_len = len(list(graphemes(sentence_noisy)))\n",
    "        sentence_len = len(list(graphemes(sentence)))\n",
    "        if sentence_noisy_len!=sentence_len:\n",
    "            continue\n",
    "        if not char_max_check(sentence):\n",
    "            continue\n",
    "\n",
    "        sentences.append(sentence)\n",
    "        sentences_noisy.append(sentence_noisy)\n",
    "    return sentences, sentences_noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c6d2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('PeOcrSanskrit/train_devnagari.csv')\n",
    "dev_df = pd.read_csv('PeOcrSanskrit/val_devnagari.csv')\n",
    "test_df = pd.read_csv('PeOcrSanskrit/test_devnagari.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7938f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208137/208137 [00:41<00:00, 5019.56it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences, sentences_noisy = preproc_df(train_df)\n",
    "train_df = pd.DataFrame({\n",
    "    'sentence': sentences,\n",
    "    'sentence_noisy': sentences_noisy,\n",
    "})  \n",
    "train_ds = datasets.Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd044cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.026987032579502922)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df['input_text'] == train_df['target_text']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efd2db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5695332739637305)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df['sentence'] == train_df['sentence_noisy']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfbe567f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_noisy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>स्वमन्यथा हल्यइत वर्ग जननाशं करोति च ।</td>\n",
       "      <td>स्वमन्यथा हल्यइत वर्ग जननाशं करोति च ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>कक्षायां प्रत्यहं याति समसूत्रानुसारतः ।</td>\n",
       "      <td>कक्षायां प्रत्यहे याति सममूत्रानुसारतः ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>तृतीयं लङ्काक्षितिज- मुन्मण्डलम्11।</td>\n",
       "      <td>तृतीयं लङ्काक्षितिज- मुन्मण्डलम्' ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>( ५६ ओर दौ'र्या मन्दकेन्द्रत्पा तिजाग्नया, सा ...</td>\n",
       "      <td>( ५६ ओर दौ'र्या मन्दकेन्द्रत्पा तिजाग्नया, सा ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>इति पञ्चविंशं सूत्रम् ।</td>\n",
       "      <td>इति पञ्चविंशं सूत्रम् ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98811</th>\n",
       "      <td>गुरोः षोडशांशाः १६ ।</td>\n",
       "      <td>गुरोः षोडशांशाः १६ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98812</th>\n",
       "      <td>तस्मादयथार्थप्रत्यवस्थानात् यत्किञ्चिदेतत् ।</td>\n",
       "      <td>तस्मादयथार्थप्रत्यवस्थानात् यत्किञ्चिदेतत् ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98813</th>\n",
       "      <td>इति भगवद्गीतोपबृंहणमुपर्युक्त एवार्थेऽनुकूलं द...</td>\n",
       "      <td>इति भगवद्कीतोयृंहणमुपर्युक्त एवार्थऽनुकूलं दृश...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98814</th>\n",
       "      <td>यदि स्वविषयत-ज्जातीयान्यवृत्तिरनैकान्तिक इति म...</td>\n",
       "      <td>यदि स्वविपयत-ज्जातीयान्यवृत्तिरनैकान्तिक इति म...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98815</th>\n",
       "      <td>तत्र जघन्येनैकः सिध्यति ।</td>\n",
       "      <td>तत्र जघन्येनैकः सिध्यति ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0                 स्वमन्यथा हल्यइत वर्ग जननाशं करोति च ।   \n",
       "1               कक्षायां प्रत्यहं याति समसूत्रानुसारतः ।   \n",
       "2                    तृतीयं लङ्काक्षितिज- मुन्मण्डलम्11।   \n",
       "3      ( ५६ ओर दौ'र्या मन्दकेन्द्रत्पा तिजाग्नया, सा ...   \n",
       "4                                इति पञ्चविंशं सूत्रम् ।   \n",
       "...                                                  ...   \n",
       "98811                               गुरोः षोडशांशाः १६ ।   \n",
       "98812       तस्मादयथार्थप्रत्यवस्थानात् यत्किञ्चिदेतत् ।   \n",
       "98813  इति भगवद्गीतोपबृंहणमुपर्युक्त एवार्थेऽनुकूलं द...   \n",
       "98814  यदि स्वविषयत-ज्जातीयान्यवृत्तिरनैकान्तिक इति म...   \n",
       "98815                          तत्र जघन्येनैकः सिध्यति ।   \n",
       "\n",
       "                                          sentence_noisy  \n",
       "0                 स्वमन्यथा हल्यइत वर्ग जननाशं करोति च ।  \n",
       "1               कक्षायां प्रत्यहे याति सममूत्रानुसारतः ।  \n",
       "2                    तृतीयं लङ्काक्षितिज- मुन्मण्डलम्' ।  \n",
       "3      ( ५६ ओर दौ'र्या मन्दकेन्द्रत्पा तिजाग्नया, सा ...  \n",
       "4                                इति पञ्चविंशं सूत्रम् ।  \n",
       "...                                                  ...  \n",
       "98811                               गुरोः षोडशांशाः १६ ।  \n",
       "98812       तस्मादयथार्थप्रत्यवस्थानात् यत्किञ्चिदेतत् ।  \n",
       "98813  इति भगवद्कीतोयृंहणमुपर्युक्त एवार्थऽनुकूलं दृश...  \n",
       "98814  यदि स्वविपयत-ज्जातीयान्यवृत्तिरनैकान्तिक इति म...  \n",
       "98815                          तत्र जघन्येनैकः सिध्यति ।  \n",
       "\n",
       "[98816 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12ee8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_ds:\n",
    "    if len(list(graphemes(data['sentence'])))!=len(list(graphemes(data['sentence_noisy']))):\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d778d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5059.79it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences, sentences_noisy = preproc_df(dev_df)\n",
    "dev_df = pd.DataFrame({\n",
    "    'sentence': sentences,\n",
    "    'sentence_noisy': sentences_noisy,\n",
    "})  \n",
    "dev_ds = datasets.Dataset.from_pandas(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25fbf9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5026.57it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences, sentences_noisy = preproc_df(test_df)\n",
    "test_df = pd.DataFrame({\n",
    "    'sentence': sentences,\n",
    "    'sentence_noisy': sentences_noisy,\n",
    "})  \n",
    "test_ds = datasets.Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b758c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_max_len = 0\n",
    "char_max = ''\n",
    "sentence_max = '' \n",
    "for sentence, sentence_noisy in zip(sentences,sentences_noisy):\n",
    "    for char in graphemes(sentence):\n",
    "        if len(char)>char_max_len:\n",
    "            char_max_len=len(char)  \n",
    "            char_max = char \n",
    "            sentence_max = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "276d0cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('तत्र स्वस्वकक्षास्थितलिप्तानां लघुमहत्त्वात् लिप्तावशेन शीघ्रमन्दत्व- मुच्चवशेन च गतीनामुपपन्नम् ।',\n",
       " 3,\n",
       " 'नां')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_max, char_max_len, char_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "565c3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'dev': dev_ds,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd962291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 99/99 [00:00<00:00, 1369.27ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 661.63ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 639.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jwengr/PeOcrSanskritPreproc/commit/1b236f2e390b139ed11f5218418b9b03fd96ac69', commit_message='Upload dataset', commit_description='', oid='1b236f2e390b139ed11f5218418b9b03fd96ac69', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jwengr/PeOcrSanskritPreproc', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jwengr/PeOcrSanskritPreproc'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(repo_id=f'jwengr/PeOcrSanskritPreproc', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d10385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(graphemes('ज्योतिष्ठोमेन')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjw1214_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
